# ğŸ§  MEGARAG  
### _"An overcomplicated way to RAG systems."_

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-Framework-009688)
![Status](https://img.shields.io/badge/status-Experimental-orange)

---

## ğŸ“– Overview

**MEGARAG** is a modular, extensible demo of a **Retrieval-Augmented Generation (RAG)** system built with **FastAPI**.  
It aims to explore the design space of **multi-stage retrieval**, **intent classification**, and **adaptive query routing** â€” all with an unapologetically over-engineered architecture.

The project currently supports both **BM25** and **dense embedding** retrieval, integrates **RAPTOR-style summarization**, and exposes its logic through a simple **FastAPI backend**.  
It is designed for experimentation, benchmarking, and extensibility rather than production deployment (for now ğŸ˜‰).

---

## ğŸ§© Core Features

- ğŸ” **Hybrid Retrieval Stack** â€” BM25 (lexical) + Dense embeddings (Chroma, etc.)  
- ğŸª¶ **RAPTOR Summarization** â€” hierarchical chunking and topic clustering for efficient retrieval  
- ğŸ§  **Query Classifier** â€” detects query intent (e.g., chitchat, general knowledge, unsafe, internal knowledge)  
- ğŸ§° **Multi-Tenant Architecture** â€” each dataset has its own vector DB directory structure  
- âš™ï¸ **FastAPI Interface** â€” REST endpoints for query, ingestion, and diagnostics  
- ğŸ§© **LLM-Agnostic** â€” supports any LLM backend (tested with **Gemma 3 1B** on **Ollama**)  
- ğŸ§ª **Modular Design** â€” each stage (indexing, cleaning, summarization, classification, retrieval) is isolated and reusable  
![Pipeline backend indexing](images/bck_indx_pipeline.png)


---

## ğŸ—ï¸ Project Structure

```
MEGARAG/
â”œâ”€â”€ back_indx/                  # Core backend logic (BM25, RAPTOR, etc.)
â”‚   â”œâ”€â”€ bm25_pipeline.py
â”‚   â”œâ”€â”€ raptor.py
â”‚   â”œâ”€â”€ indx_pipeline.py
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ api/                        # FastAPI routes and server entry point
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ models/                     # Query classifier weights or LLM configs
â”‚   â””â”€â”€ best/
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/MelvynChemin/MEGARAG.git
cd MEGARAG
```

### 2ï¸âƒ£ Create a virtual environment

```bash
python3 -m venv megarag
source megarag/bin/activate
```

### 3ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Launch the FastAPI backend

```bash
uvicorn api.main:app --reload
```

API will be available at:

> [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

---

## ğŸ§  Example Usage

### Query an indexed dataset

```bash
curl -X POST "http://127.0.0.1:8000/query" \
     -H "Content-Type: application/json" \
     -d '{"query": "Explain RAG retrieval flow"}'
```

### Ingest new documents

```bash
curl -X POST "http://127.0.0.1:8000/ingest" \
     -H "Content-Type: application/json" \
     -d '{"path": "data/my_docs"}'
```

---

## ğŸ§© Supported LLMs

MEGARAG is **LLM-agnostic** â€” you can connect:

* ğŸ¦™ **Ollama** (tested with Google **Gemma 3 1B**)
* ğŸ”¥ **OpenAI GPT** (via API key)
* ğŸ§± **Local models** (e.g., Mistral, Llama 3, DeepSeek, etc.)

Configuration can be adapted in your environment or inside `api/main.py`.

---

## âš™ï¸ Tech Stack

| Component     | Technology                                |
| ------------- | ----------------------------------------- |
| API Layer     | FastAPI                                   |
| Retrieval     | BM25 (rank_bm25), Chroma                  |
| Summarization | RAPTOR-style hierarchical nodes           |
| Vectorization | Sentence Transformers / OpenAI Embeddings |
| LLM Interface | Ollama / OpenAI API                       |
| Orchestration | Python 3.10+, Modular scripts             |

---

## ğŸ§­ Roadmap

* [ ] Add front-end dashboard
* [ ] Add support for mixed-context RAG (structured + unstructured)
* [ ] Implement caching layer (FAISS/Redis)
* [ ] Integrate evaluation metrics (Recall@k, MRR, etc.)
* [ ] Add Dockerfile for easy deployment

---

## ğŸ§‘â€ğŸ’» Author

**Melvyn CHEMIN**  
Master's in AI & Data Science @ ENSIMAG  
Currently building modular multi-agent and RAG systems.  
[ğŸ”— LinkedIn](https://www.linkedin.com/in/melvynchemin) â€¢ [ğŸ™ GitHub](https://github.com/MelvynChemin)

---

## âš–ï¸ License

MIT License Â© 2025 Melvyn CHEMIN
